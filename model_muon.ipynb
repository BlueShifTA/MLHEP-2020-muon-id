{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Coopetition - Muon id classification \n",
    "\n",
    "S. Ek-In, C. Praz\n",
    "\n",
    "\n",
    "[x] Select features and find new features \\\n",
    "[ ] Add Scaling to wide range variables \\\n",
    "[ ] CatBoost \\\n",
    "[x] Scale weight - NOT USED\n",
    "\n",
    "-> Need the module swifter with fsspec==0.3.3 , if the version is newer than this, the code might break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Import part \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_scorer(get_rejection_at_efficiency, needs_threshold=True, threshold=0.9)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "import utils\n",
    "import scoring\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# The datasets are available in CoCalc in ~/share/data/I-coopetition-muon-id/\n",
    "# Test\n",
    "# ! wget --content-disposition https://codalab.coresearch.club/my/datasets/download/dd6255a1-a14b-4276-9a2b-db7f360e01c7\n",
    "# Train\n",
    "# ! wget --content-disposition https://codalab.coresearch.club/my/datasets/download/3a5e940c-2382-4716-9ff7-8fbc269b98ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600000, 76)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"~/share/data/I-coopetition-muon-id/\"\n",
    "columns = utils.SIMPLE_FEATURE_COLUMNS + [\"id\", \"label\", \"weight\", \"sWeight\", \"kinWeight\"]\n",
    "train = utils.load_full_train_csv(DATA_PATH, 600000)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 2 Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Main Setting \n",
    "\n",
    "weight_name = \"weight\"\n",
    "num_sample = 10\n",
    "train_vis = train.head(num_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "'''\n",
    "    Utils functions\n",
    "'''\n",
    "\n",
    "#classes = [0, 1]\n",
    "#columns = ['P', 'PT','ncl[0]', 'avg_cs[0]', 'ndof', 'MatchedHit_TYPE[0]', 'MatchedHit_TYPE[1]', 'MatchedHit_TYPE[2]']\n",
    "\n",
    "\n",
    "def visualize(feature, target, weights, num_bins=100):\n",
    "    classes = np.unique(target)\n",
    "    bins = np.linspace(feature.min(), feature.max(), num_bins + 1)\n",
    "    \n",
    "    # Plot all class \n",
    "    for c in classes:\n",
    "        selection = (target == c)\n",
    "        plt.hist(feature[selection], bins = bins, label = c, alpha = 0.5, weights = weights[selection])\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ncl[0]', 'ncl[1]', 'ncl[2]', 'ncl[3]', 'avg_cs[0]', 'avg_cs[1]',\n",
      "       'avg_cs[2]', 'avg_cs[3]', 'ndof', 'MatchedHit_TYPE[0]',\n",
      "       'MatchedHit_TYPE[1]', 'MatchedHit_TYPE[2]', 'MatchedHit_TYPE[3]',\n",
      "       'MatchedHit_X[0]', 'MatchedHit_X[1]', 'MatchedHit_X[2]',\n",
      "       'MatchedHit_X[3]', 'MatchedHit_Y[0]', 'MatchedHit_Y[1]',\n",
      "       'MatchedHit_Y[2]', 'MatchedHit_Y[3]', 'MatchedHit_Z[0]',\n",
      "       'MatchedHit_Z[1]', 'MatchedHit_Z[2]', 'MatchedHit_Z[3]',\n",
      "       'MatchedHit_DX[0]', 'MatchedHit_DX[1]', 'MatchedHit_DX[2]',\n",
      "       'MatchedHit_DX[3]', 'MatchedHit_DY[0]', 'MatchedHit_DY[1]',\n",
      "       'MatchedHit_DY[2]', 'MatchedHit_DY[3]', 'MatchedHit_DZ[0]',\n",
      "       'MatchedHit_DZ[1]', 'MatchedHit_DZ[2]', 'MatchedHit_DZ[3]',\n",
      "       'MatchedHit_T[0]', 'MatchedHit_T[1]', 'MatchedHit_T[2]',\n",
      "       'MatchedHit_T[3]', 'MatchedHit_DT[0]', 'MatchedHit_DT[1]',\n",
      "       'MatchedHit_DT[2]', 'MatchedHit_DT[3]', 'Lextra_X[0]', 'Lextra_X[1]',\n",
      "       'Lextra_X[2]', 'Lextra_X[3]', 'Lextra_Y[0]', 'Lextra_Y[1]',\n",
      "       'Lextra_Y[2]', 'Lextra_Y[3]', 'NShared', 'Mextra_DX2[0]',\n",
      "       'Mextra_DX2[1]', 'Mextra_DX2[2]', 'Mextra_DX2[3]', 'Mextra_DY2[0]',\n",
      "       'Mextra_DY2[1]', 'Mextra_DY2[2]', 'Mextra_DY2[3]', 'FOI_hits_N',\n",
      "       'FOI_hits_X', 'FOI_hits_Y', 'FOI_hits_Z', 'FOI_hits_DX', 'FOI_hits_DY',\n",
      "       'FOI_hits_T', 'FOI_hits_S', 'PT', 'P', 'sWeight', 'label', 'kinWeight',\n",
      "       'weight'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f61208ea414328adc6d269eab5cd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=600000.0, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "execution_count": 7,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Looking for high level parameters \n",
    "import swifter \n",
    "def Get_closest_hits(data):\n",
    "    \n",
    "    closest_hits_features = data.swifter.apply(utils.find_closest_hit_per_station, result_type=\"expand\", axis=1)\n",
    "    closest_hits_features.columns = [\"closest_{}\".format(ind) for ind in range(len(closest_hits_features.columns))]\n",
    "    \n",
    "    return closest_hits_features\n",
    "\n",
    "close_hits = Get_closest_hits(train)\n",
    "train_mod = pd.concat([train, close_hits], axis = 1)\n",
    "\n",
    "# Save to files for backing up\n",
    "close_hits.to_pickle('train_closest_hit.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Scale product of sWeight\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "def scale_sweight(data, edge_weight = None):\n",
    "    if edge_weight != None:\n",
    "        data_mod = data[data['weight'].abs() < edge_weight]\n",
    "        data_mod['scale_weight'] = (data['weight'] + edge_weight)/(2 * edge_weight)\n",
    "        data_mod['scale_weight'] = data_mod['scale_weight'] * (data_mod['weight'].sum() / data_mod['scale_weight'].sum())\n",
    "    \n",
    "        return data_mod\n",
    "    else:\n",
    "        data_mod['scale_weight'] = (data_mod['weight'] + data_mod['weight'].min()) (data_mod['weight'].sum() / data_mod['scale_weight'].sum())\n",
    "\n",
    "# Scale sweight\n",
    "#train_mod = scale_sweight(train_mod, edge_weight = None)\n",
    "    \n",
    "# Add new features \n",
    "def feat_PZ(data): \n",
    "    return np.sqrt(data['P'] ** 2  - data['PT'] ** 2)\n",
    "\n",
    "\n",
    "\n",
    "train_mod['PZ'] = feat_PZ(train_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0         1.026119\n",
       "1         1.012146\n",
       "2         1.020520\n",
       "3         1.003271\n",
       "4         1.003093\n",
       "            ...   \n",
       "599995    1.045375\n",
       "599996    1.010911\n",
       "599997    1.002369\n",
       "599998    1.002907\n",
       "599999    1.023048\n",
       "Length: 600000, dtype: float32"
      ]
     },
     "execution_count": 28,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mod['P']/train_mod['PZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Visualisation \n",
    "\n",
    "for column in []:\n",
    "    plt.figure()\n",
    "    visualize(train[column][:num_sample], train['label'][:num_sample], train[weight_name][:num_sample])\n",
    "    plt.title(column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Note: \n",
    " - Acc = 0.70, train_cols with close hit, add PZ, Model: XGBoost LR = 0.1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Def used columns \n",
    "train_cols = ['ncl[0]', 'ncl[1]', 'ncl[2]', 'ncl[3]', 'avg_cs[0]', 'avg_cs[1]',\n",
    "       'avg_cs[2]', 'avg_cs[3]', 'ndof', 'MatchedHit_TYPE[0]',\n",
    "       'MatchedHit_TYPE[1]', 'MatchedHit_TYPE[2]', 'MatchedHit_TYPE[3]',\n",
    "       'MatchedHit_X[0]', 'MatchedHit_X[1]', 'MatchedHit_X[2]',\n",
    "       'MatchedHit_X[3]', 'MatchedHit_Y[0]', 'MatchedHit_Y[1]',\n",
    "       'MatchedHit_Y[2]', 'MatchedHit_Y[3]', 'MatchedHit_Z[0]',\n",
    "       'MatchedHit_Z[1]', 'MatchedHit_Z[2]', 'MatchedHit_Z[3]',\n",
    "       'MatchedHit_DX[0]', 'MatchedHit_DX[1]', 'MatchedHit_DX[2]',\n",
    "       'MatchedHit_DX[3]', 'MatchedHit_DY[0]', 'MatchedHit_DY[1]',\n",
    "       'MatchedHit_DY[2]', 'MatchedHit_DY[3]', 'MatchedHit_DZ[0]',\n",
    "       'MatchedHit_DZ[1]', 'MatchedHit_DZ[2]', 'MatchedHit_DZ[3]',\n",
    "       'MatchedHit_T[0]', 'MatchedHit_T[1]', 'MatchedHit_T[2]',\n",
    "       'MatchedHit_T[3]', 'MatchedHit_DT[0]', 'MatchedHit_DT[1]',\n",
    "       'MatchedHit_DT[2]', 'MatchedHit_DT[3]', 'NShared', 'Mextra_DX2[0]',\n",
    "       'Mextra_DX2[1]', 'Mextra_DX2[2]', 'Mextra_DX2[3]', 'Mextra_DY2[0]',\n",
    "       'Mextra_DY2[1]', 'Mextra_DY2[2]', 'Mextra_DY2[3]', 'FOI_hits_N',\n",
    "       'PT', 'PZ'] + close_hits.columns.tolist()\n",
    "target_col = ['label', 'weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import re\n",
    "\n",
    "def prepare_data(data):\n",
    "    regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "    data.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in data.columns.values]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_val = scaler.transform(X_val)\\npca = PCA()\\nX_train = pca.fit_transform(X_train)\\nX_val = pca.transform(X_val)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Heading data\n",
    "train.head()\n",
    "X_mod, y_mod = train_mod[train_cols], train_mod[target_col]\n",
    "\n",
    "# Rename - ignore [] \n",
    "prepare_data(X_mod)\n",
    "\n",
    "# Splitting \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_mod, y_mod, test_size=0.25, shuffle=True, random_state=2342234)\n",
    "\n",
    "# Scale \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "'''\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "pca = PCA()\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_val = pca.transform(X_val)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ncl_0_  ncl_1_  ncl_2_  ncl_3_  avg_cs_0_  avg_cs_1_  avg_cs_2_  \\\n",
      "id                                                                        \n",
      "71278     44.0     6.0    11.0     6.0   2.704545   1.666667   1.363636   \n",
      "357995   102.0    16.0     4.0    13.0   3.303921   3.812500   1.000000   \n",
      "547395    25.0     8.0    18.0    14.0   1.520000   1.375000   1.944444   \n",
      "533566    87.0    12.0     9.0     6.0   2.620690   1.500000   1.000000   \n",
      "17230    102.0    39.0    22.0    11.0   2.205882   2.384615   1.272727   \n",
      "...        ...     ...     ...     ...        ...        ...        ...   \n",
      "373825    33.0     4.0     7.0    11.0   4.303030   2.500000   1.000000   \n",
      "275161    79.0    39.0    14.0    13.0   2.050633   2.461539   1.285714   \n",
      "171712    66.0    15.0     7.0    11.0   1.818182   1.200000   1.142857   \n",
      "533939    15.0    11.0     5.0    11.0   2.200000   2.363636   1.400000   \n",
      "388600    82.0    18.0     7.0    12.0   2.134146   2.277778   1.285714   \n",
      "\n",
      "        avg_cs_3_  ndof  MatchedHit_TYPE_0_  ...    closest_14    closest_15  \\\n",
      "id                                           ...                               \n",
      "71278    1.166667   8.0                 2.0  ...  17722.027344  18930.929688   \n",
      "357995   1.384615   8.0                 2.0  ...  17510.126953  19002.253906   \n",
      "547395   1.571429   8.0                 2.0  ...  17726.908203  18936.144531   \n",
      "533566   1.166667   4.0                 2.0  ...   1000.000000   1000.000000   \n",
      "17230    1.090909   8.0                 2.0  ...  17516.980469  18725.216797   \n",
      "...           ...   ...                 ...  ...           ...           ...   \n",
      "373825   1.000000   8.0                 2.0  ...  17720.310547  18929.101562   \n",
      "275161   1.230769   8.0                 2.0  ...  17604.763672  18813.652344   \n",
      "171712   1.090909   8.0                 2.0  ...  17720.640625  19006.992188   \n",
      "533939   1.454545   8.0                 2.0  ...  17710.548828  18918.279297   \n",
      "388600   1.416667   8.0                 1.0  ...  17808.416016  18815.541016   \n",
      "\n",
      "        closest_16  closest_17   closest_18   closest_19  closest_20  \\\n",
      "id                                                                     \n",
      "71278     6.375000    6.875000    29.500000    31.500000   31.479162   \n",
      "357995   12.750000   13.750000    59.000000    63.000000   63.078957   \n",
      "547395   12.750000   13.750000    59.000000    63.000000   63.078957   \n",
      "533566   12.750000   82.500000  1000.000000  1000.000000   63.078957   \n",
      "17230     3.208333    3.458333    14.833333    15.833333   15.679264   \n",
      "...            ...         ...          ...          ...         ...   \n",
      "373825   25.500000   27.500000   118.000000   126.000000  126.278549   \n",
      "275161    6.375000    6.875000    29.500000    31.500000   31.479162   \n",
      "171712    3.208333    3.458333    14.833333    15.833333   15.679264   \n",
      "533939   12.750000   13.750000    59.000000    63.000000   63.078957   \n",
      "388600   19.250000    3.458333    29.500000    15.833333   15.679264   \n",
      "\n",
      "        closest_21   closest_22   closest_23  \n",
      "id                                            \n",
      "71278    33.979145    36.479130    38.979115  \n",
      "357995   68.078926    73.078896    78.078857  \n",
      "547395   68.078926    73.078896    78.078857  \n",
      "533566   33.979145  1000.000000  1000.000000  \n",
      "17230    16.929256    18.179249    19.429239  \n",
      "...            ...          ...          ...  \n",
      "373825  136.278488   146.278412   156.278351  \n",
      "275161   33.979145    36.479130    38.979115  \n",
      "171712   16.929256    18.179249    19.429239  \n",
      "533939   68.078926    73.078896    78.078857  \n",
      "388600   16.929256    36.479130    19.429239  \n",
      "\n",
      "[450000 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 3 Training part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Model Arch\n",
    "n_trees = 100\n",
    "model = xgboost.XGBClassifier(n_estimators = n_trees, nthread=-1, learning_rate = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainning with lr = 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: 0.1 , Test accuracy: 0.7057152573566184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Model_Best.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "# Trainning part\n",
    "\n",
    "score_best = 0\n",
    "model_best = None\n",
    "lr_best    = None\n",
    "\n",
    "lrs = [0.1]\n",
    "\n",
    "for lr in lrs:\n",
    "    print(\"trainning with lr = {}\".format(lr))\n",
    "    model = xgboost.XGBClassifier(n_estimators = n_trees, nthread=-1, learning_rate = lr)\n",
    "\n",
    "    model.fit(X_train.iloc[:, :],\n",
    "              y_train['label'].values,\n",
    "              #sample_weight=y_train.scale_weight.values,\n",
    "              #verbose=True,\n",
    "             )\n",
    "    ''' train with scaler and PCA \n",
    "    \n",
    "    model.fit(X_train,\n",
    "              y_train['label'].values,\n",
    "              sample_weight=y_train.scale_weight.values,\n",
    "              verbose=True,\n",
    "             )\n",
    "    '''\n",
    "    validation_predictions = model.predict_proba(X_val)[:, 1]\n",
    "    #model_score = scoring.rejection90(y_val.label.values, validation_predictions, sample_weight = y_val.scale_weight.values)\n",
    "    model_score = scoring.rejection90(y_val.label.values, validation_predictions)\n",
    "\n",
    "\n",
    "    print(\"NN: {} , Test accuracy: {}\".format(lr, model_score))\n",
    "    if model_score > score_best :\n",
    "        model_best = model\n",
    "        score_best = model_score\n",
    "        lr_best    = lr\n",
    "\n",
    "# Save Model \n",
    "joblib.dump(model_best, 'Model_Best.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7057152573566184\n"
     ]
    }
   ],
   "source": [
    "print(score_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "validation_predictions = model.predict_proba(X_val)[:, 1]\n",
    "model_score = scoring.rejection90(y_val.label.values, validation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5a345b32fc4fc49421dbdc04006822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=120000.0, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "execution_count": 21,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Further test on 20% of original training \n",
    "Eval_train = train.head(int(0.5 * train.shape[0]))\n",
    "\n",
    "# Prep sample \n",
    "close_hits = Get_closest_hits(Eval_train)\n",
    "Eval_train = pd.concat([Eval_train, close_hits], axis = 1)\n",
    "Eval_train['PZ'] = feat_PZ(Eval_train)\n",
    "\n",
    "Eval_X = Eval_train[train_cols]\n",
    "prepare_data(Eval_X)\n",
    "\n",
    "\n",
    "# Predict and save file \n",
    "Eval_true =  Eval_train[['label']]\n",
    "predictions = model_best.predict_proba(Eval_X)[:, 1]\n",
    "model_score = scoring.rejection90(Eval_true.label.values, predictions, Eval_train.weight.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.730293198462288\n"
     ]
    }
   ],
   "source": [
    "print(model_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 4 Predict on the whole test set and prepare submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "# Read data\n",
    "DATA_PATH = \"~/share/data/I-coopetition-muon-id/\"\n",
    "test = utils.load_full_test_csv(DATA_PATH, None)\n",
    "\n",
    "\n",
    "# Transform data \n",
    "close_hits_test = Get_closest_hits(test)\n",
    "test_mod = pd.concat([test, close_hits_test], axis = 1)\n",
    "\n",
    "test_mod['PZ'] = feat_PZ(test_mod)\n",
    "X_test = test_mod[train_cols]\n",
    "prepare_data(X_test)\n",
    "\n",
    "\n",
    "# Predict and save file \n",
    "predictions = model_best.predict_proba(X_test)[:, 1]\n",
    "\n",
    "compression_opts = dict(method='zip',\n",
    "                        archive_name='submission.csv')  \n",
    "pd.DataFrame(data={\"prediction\": predictions}, index=test.index).to_csv(\n",
    "    \"submission.zip\", index_label=utils.ID_COLUMN, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}